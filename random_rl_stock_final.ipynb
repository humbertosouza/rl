{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_rl_stock_final",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humbertosouza/rl/blob/master/random_rl_stock_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM75kPHJ1X4B",
        "colab_type": "text"
      },
      "source": [
        "**Random RL Stock script**\n",
        "\n",
        "It is basically the same script. However, it replaces the agent witha random action generator.\n",
        "\n",
        "It is done in the class play_one_episode.\n",
        "\n",
        "The subfolder names hold the name 'linear_rl_trader_random_*'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uoZ3LYJzgXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgPYyB4pVCJZ",
        "colab_type": "text"
      },
      "source": [
        "**Auxiliary non-core functions**\n",
        "\n",
        "The following functions handles the csv datasource, folder creation and the scaler for the linear model preparation.\n",
        "\n",
        "Scaler is of utmost importance to balance very different variables such as asset price, cash available and number of stocks held.\n",
        "\n",
        "The datasource is expected to be already 'clean'\n",
        "\n",
        "It is handled differently when running from colab or from a server (to be called from an API). Refer to 'Main Section' to know more.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZJhprmXVCo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Auxiliary functions (non-core)\n",
        "# ==============================\n",
        "\n",
        "# The stock Reference is AAPL (Apple), MSI (Motorola), SBUX (Starbucks)\n",
        "# returns a T x 3 list of stock prices\n",
        "# each row is a different stock\n",
        "# 0 = AAPL\n",
        "# 1 = MSI\n",
        "# 2 = SBUX\n",
        "# You may use 'https://github.com/humbertosouza/rl/blob/master/assignment4/aapl_msi_sbux.csv'\n",
        "#\n",
        "# *** OR ***\n",
        "# The asset reference will be the BTCUSD price\n",
        "#  you may use 'https://github.com/humbertosouza/rl/blob/master/assignment4/BTCUSDT-4h-data-pure.csv'\n",
        "\n",
        "def get_data(url):\n",
        "  df = pd.read_csv(url)\n",
        "  return df.values\n",
        "\n",
        "# return scikit-learn scaler object to scale the states\n",
        "# Note: you could also populate the replay buffer here\n",
        "def get_scaler(env):\n",
        "\n",
        "  states = []\n",
        "  for _ in range(env.n_step):\n",
        "    action = np.random.choice(env.action_space)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    states.append(state)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(states)\n",
        "  return scaler\n",
        "\n",
        "# Creates the foder 'directory' if it does not exist\n",
        "def maybe_make_dir(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Creates the header of a text file for the rewards\n",
        "# Useful for analysis of the hyperparameters\n",
        "def write_txt_header(reward_txt_file, num_episodes,batch_size,initial_investment):\n",
        "  f=open(reward_txt_file,\"a+\")\n",
        "  f.write(\"-----------------------------------------------\\r\\n\")\n",
        "  f.write(\"----------- Starting new cycle ----------------\\r\\n\")\n",
        "  f.write(\"-----------------------------------------------\\r\\n\")\n",
        "  txt_now = f'{datetime.now()}'\n",
        "  f.write(txt_now)\n",
        "  f.write(\"\\r\\n\")\n",
        "  f.write(f'Paramenters: num_episodes:{num_episodes}; batch_size:{batch_size}; initial_investiment:{initial_investment}')\n",
        "  f.write(\"\\r\\n\")\n",
        "  f.close()\n",
        "\n",
        "\n",
        "# End of the Auxiliary functions (non-core)\n",
        "# ========================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXZF5eXbUmBy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**SGD Linear Model Class**\n",
        "\n",
        "The Linear Model implements the functions \n",
        "\n",
        "- Predict\n",
        "- sgd\n",
        "\n",
        "The sgd method implements momentum (W,b) to make the model to cope with the latest trend agains the older ones. It relies on a one-step descent.\n",
        "\n",
        "**SGL Linear Model auxiliary functions**\n",
        "\n",
        "In order to make it faster \n",
        "\n",
        "It also contains the auxiliary functions\n",
        "- load_weights (from a numpy file)\n",
        "- save_weights (to a numpy file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvwaf3s_Umee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implements manually a Linear Model class using Stochastic Gradient Descent\n",
        "class LinearModel:\n",
        "  \"\"\" A linear regression model \"\"\"\n",
        "  def __init__(self, input_dim, n_action):\n",
        "    self.W = np.random.randn(input_dim, n_action) / np.sqrt(input_dim)\n",
        "    self.b = np.zeros(n_action)\n",
        "\n",
        "    # momentum terms\n",
        "    self.vW = 0\n",
        "    self.vb = 0\n",
        "\n",
        "    self.losses = []\n",
        "\n",
        "  def predict(self, X):\n",
        "    # make sure X is N x D\n",
        "    assert(len(X.shape) == 2)\n",
        "    return X.dot(self.W) + self.b\n",
        "\n",
        "  def sgd(self, X, Y, learning_rate=0.01, momentum=0.9):\n",
        "    # make sure X is N x D\n",
        "    assert(len(X.shape) == 2)\n",
        "\n",
        "    # the loss values are 2-D\n",
        "    # normally we would divide by N only\n",
        "    # but now we divide by N x K\n",
        "    num_values = np.prod(Y.shape)\n",
        "\n",
        "    # do one step of gradient descent\n",
        "    # we multiply by 2 to get the exact gradient\n",
        "    # (not adjusting the learning rate)\n",
        "    # i.e. d/dx (x^2) --> 2x\n",
        "    Yhat = self.predict(X)\n",
        "    gW = 2 * X.T.dot(Yhat - Y) / num_values\n",
        "    gb = 2 * (Yhat - Y).sum(axis=0) / num_values\n",
        "\n",
        "    # update momentum terms\n",
        "    self.vW = momentum * self.vW - learning_rate * gW\n",
        "    self.vb = momentum * self.vb - learning_rate * gb\n",
        "\n",
        "    # update params\n",
        "    self.W += self.vW\n",
        "    self.b += self.vb\n",
        "\n",
        "    mse = np.mean((Yhat - Y)**2)\n",
        "    self.losses.append(mse)\n",
        "\n",
        "  # Load weights from Numpy file\n",
        "  def load_weights(self, filepath):\n",
        "    npz = np.load(filepath)\n",
        "    self.W = npz['W']\n",
        "    self.b = npz['b']\n",
        "    \n",
        "  # Save weights to Numpy file\n",
        "  def save_weights(self, filepath):\n",
        "    np.savez(filepath, W=self.W, b=self.b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzZpVdmeT1p2",
        "colab_type": "text"
      },
      "source": [
        "**Multi stock environment class** (3 classes)\n",
        "\n",
        "This class reads the EOD stock prices and makes a decision about what to do with each of them:\n",
        "0. Sell\n",
        "1. Buy\n",
        "2. Hold\n",
        "\n",
        "The environment itself is a vector which contains 7 variables.\n",
        "\n",
        "[assetOwned1,assetOwned2,assetOwned3,priceAsset1,priceAsset2,priceAsset3,cashOwned]\n",
        "\n",
        "It also implements reset which is important for reinitialize the environment and start the next iteration.\n",
        "\n",
        "The environment also implements the step function. It performs the needed updates to provide the agent with the results of all variables in the environment. It also conforms with th OpenAI gym API.\n",
        "\n",
        "The method trade performs the buys and sells accoding to the actions to be taken. It is used in the main method  to populate the env variable that feeds the agent. If under test, it is reloaded with the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSlTxLl6T2D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Original class that handles 3-stocks at once\n",
        "class MultiStockEnv:\n",
        "  \"\"\"\n",
        "  A 3-stock trading environment.\n",
        "  State: vector of size 7 (n_stock * 2 + 1)\n",
        "    - # shares of stock 1 owned\n",
        "    - # shares of stock 2 owned\n",
        "    - # shares of stock 3 owned\n",
        "    - price of stock 1 (using daily close price)\n",
        "    - price of stock 2\n",
        "    - price of stock 3\n",
        "    - cash owned (can be used to purchase more stocks)\n",
        "  Action: categorical variable with 27 (3^3) possibilities\n",
        "    - for each stock, you can:\n",
        "    - 0 = sell\n",
        "    - 1 = hold\n",
        "    - 2 = buy\n",
        "  \"\"\"\n",
        "  def __init__(self, data, initial_investment=20000):\n",
        "    # data\n",
        "    self.stock_price_history = data\n",
        "    self.n_step, self.n_stock = self.stock_price_history.shape\n",
        "\n",
        "    # instance attributes\n",
        "    self.initial_investment = initial_investment\n",
        "    self.cur_step = None\n",
        "    self.stock_owned = None\n",
        "    self.stock_price = None\n",
        "    self.cash_in_hand = None\n",
        "\n",
        "    self.action_space = np.arange(3**self.n_stock)\n",
        "\n",
        "    # action permutations\n",
        "    # returns a nested list with elements like:\n",
        "    # [0,0,0]\n",
        "    # [0,0,1]\n",
        "    # [0,0,2]\n",
        "    # [0,1,0]\n",
        "    # [0,1,1]\n",
        "    # etc.\n",
        "    # 0 = sell\n",
        "    # 1 = hold\n",
        "    # 2 = buy\n",
        "    self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
        "\n",
        "    # calculate size of state\n",
        "    self.state_dim = self.n_stock * 2 + 1\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.cur_step = 0\n",
        "    self.stock_owned = np.zeros(self.n_stock)\n",
        "    self.stock_price = self.stock_price_history[self.cur_step]\n",
        "    self.cash_in_hand = self.initial_investment\n",
        "    return self._get_obs()\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    assert action in self.action_space\n",
        "\n",
        "    # get current value before performing the action\n",
        "    prev_val = self._get_val()\n",
        "\n",
        "    # update price, i.e. go to the next day\n",
        "    self.cur_step += 1\n",
        "    self.stock_price = self.stock_price_history[self.cur_step]\n",
        "\n",
        "    # perform the trade\n",
        "    self._trade(action)\n",
        "\n",
        "    # get the new value after taking the action\n",
        "    cur_val = self._get_val()\n",
        "\n",
        "    # reward is the increase in porfolio value\n",
        "    reward = cur_val - prev_val\n",
        "\n",
        "    # done if we have run out of data\n",
        "    done = self.cur_step == self.n_step - 1\n",
        "\n",
        "    # store the current value of the portfolio here\n",
        "    info = {'cur_val': cur_val}\n",
        "\n",
        "    # conform to the Gym API\n",
        "    return self._get_obs(), reward, done, info\n",
        "\n",
        "\n",
        "  def _get_obs(self):\n",
        "    obs = np.empty(self.state_dim)\n",
        "    obs[:self.n_stock] = self.stock_owned\n",
        "    obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
        "    obs[-1] = self.cash_in_hand\n",
        "    return obs\n",
        "    \n",
        "\n",
        "\n",
        "  def _get_val(self):\n",
        "    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "\n",
        "  def _trade(self, action):\n",
        "    # index the action we want to perform\n",
        "    # 0 = sell\n",
        "    # 1 = hold\n",
        "    # 2 = buy\n",
        "    # e.g. [2,1,0] means:\n",
        "    # buy first stock\n",
        "    # hold second stock\n",
        "    # sell third stock\n",
        "    action_vec = self.action_list[action]\n",
        "\n",
        "    # determine which stocks to buy or sell\n",
        "    sell_index = [] # stores index of stocks we want to sell\n",
        "    buy_index = [] # stores index of stocks we want to buy\n",
        "    for i, a in enumerate(action_vec):\n",
        "      if a == 0:\n",
        "        sell_index.append(i)\n",
        "      elif a == 2:\n",
        "        buy_index.append(i)\n",
        "\n",
        "    # sell any stocks we want to sell\n",
        "    # then buy any stocks we want to buy\n",
        "    if sell_index:\n",
        "      # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
        "      for i in sell_index:\n",
        "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "        self.stock_owned[i] = 0\n",
        "    if buy_index:\n",
        "      # NOTE: when buying, we will loop through each stock we want to buy,\n",
        "      #       and buy one share at a time until we run out of cash\n",
        "      can_buy = True\n",
        "      while can_buy:\n",
        "        for i in buy_index:\n",
        "          if self.cash_in_hand > self.stock_price[i]:\n",
        "            self.stock_owned[i] += 1 # buy one share\n",
        "            self.cash_in_hand -= self.stock_price[i]\n",
        "          else:\n",
        "            can_buy = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2I0v7iPTAVU",
        "colab_type": "text"
      },
      "source": [
        "**Agent Class**\n",
        "\n",
        "This class implements the agent that relies on the linear model to define the error amongst the actions taken in a given state. Q(s,;)\n",
        "It is a e-greedy approach with a decaying factor and minimum defined in its __init__ function.\n",
        "The RL policy will vary according to the iterations ineach batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnX-eQBTBBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent(object):\n",
        "  def __init__(self, state_size, action_size, gamma=0.95, epsilon=1.0):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.gamma = gamma  # discount rate\n",
        "    self.epsilon = epsilon  # exploration rate\n",
        "    self.epsilon_min = 0.05\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.model = LinearModel(state_size, action_size)\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return np.random.choice(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "\n",
        "  def train(self, state, action, reward, next_state, done):\n",
        "    if done:\n",
        "      target = reward\n",
        "    else:\n",
        "      target = reward + self.gamma * np.amax(self.model.predict(next_state), axis=1)\n",
        "\n",
        "    target_full = self.model.predict(state)\n",
        "    target_full[0, action] = target\n",
        "\n",
        "    # Run one training step\n",
        "    self.model.sgd(state, target_full)\n",
        "\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s0CdE_0TI1V",
        "colab_type": "text"
      },
      "source": [
        "**Play one episode class**\n",
        "\n",
        "The class where a full state is handled.\n",
        "\n",
        "It gets the current state of the environment, rescales.\n",
        "\n",
        "Then it will play the episode until it is done.\n",
        "In the loop,the agent takes an action based on the current state.\n",
        "\n",
        "Then the action is applied to the environment via env.step. It returns the tuple next_state,reward,done and info.\n",
        "\n",
        "Inside the loop it is also verified if we are in train mode. If so, it calls the agent.train.\n",
        "\n",
        "Finally, the last action of the loop is apply the state to the recently acquired next_state variable.\n",
        "\n",
        "The function returns the curent portfolio value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAEkUpYzTJZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_one_episode(agent, env, is_train):\n",
        "  # note: after transforming states are already 1xD\n",
        "  state = env.reset()\n",
        "  state = scaler.transform([state])\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    #action = agent.act(state)\n",
        "    action = np.random.choice(env.action_space) # Use random for comparison\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = scaler.transform([next_state])\n",
        "    if is_train == 'train':\n",
        "      agent.train(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "  return info['cur_val']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57fWHNPe1SuR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PErv-gFO1Uey",
        "colab_type": "text"
      },
      "source": [
        "**Main section**\n",
        "\n",
        "The next section is the \"main\" section when converting it to run under a server, as a .py file.\n",
        "\n",
        "> The 'Main'  does the following:\n",
        "\n",
        "1. Defines in which folder the models will be saved\n",
        "2. Defines the folder where the rewards will be saved  \n",
        "3. Check if it is train or test\n",
        "4.   Load the dataset\n",
        "5.   Define # of episodes, batch size and initial investment\n",
        "5. split the data in train and test (dividing the dataset in 2)\n",
        "6.   Loads the environment, agent and run train or test\n",
        "8. Instantiate the objects using the parameters loaded into the variables\n",
        "7.   Save the train (pandas) file for future usage\n",
        "8.   Plot the chart***\n",
        "\n",
        "The train /test cycle is expected to happen after every new cycle. In this case, new EOD - end of day trigger)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlBbE2UY00UN",
        "colab_type": "code",
        "outputId": "ff133c50-5b25-40fa-8a48-060856cb785b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adapted to run via Colab\n",
        "# ------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# For tests, adjusts and evaluation require the files:\n",
        "#\n",
        "# BTCUSDT-4h-data-pure.csv\n",
        "#  *or*\n",
        "# aapl_msi_sbux.csv\n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "# (gamma, epslon, mode)\n",
        "# The parameters vary between 1 and 0\n",
        "# Mode can be train or test\n",
        "exploreHyper = [\n",
        "    (0.95,0.95,'train'),(0.95,0.95,'test'),\n",
        "    (0.95,0.6,'train'),(0.95,0.6,'test'),\n",
        "  #  (0.75,0.95,'train'),(0.75,0.95,'test'),\n",
        "  #  (0.75,0.6,'train'),(0.75,0.6,'test'),\n",
        "  #  (0.55,0.95,'train'),(0.55,0.95,'test'),\n",
        "  #  (0.55,0.6,'train'),(0.55,0.6,'test'),\n",
        "  #  (0.45,0.95,'train'),(0.45,0.95,'test'),\n",
        "  #  (0.45,0.6,'train'),(0.45,0.6,'test'),\n",
        "]  \n",
        "  \n",
        "# config\n",
        "num_episodes = 2000\n",
        "batch_size = 16\n",
        "initial_investment = 20000\n",
        "# Select source file\n",
        "#file = f'/content/drive/My Drive/Colab Notebooks/rl/final/BTCUSDT-4h-data-pure.csv'\n",
        "file = f'/content/drive/My Drive/Colab Notebooks/rl/final/aapl_msi_sbux.csv' \n",
        "\n",
        "# Report text file containing the rewards from different parameters\n",
        "reward_txt_file = f'/content/drive/My Drive/Colab Notebooks/rl/final/rewards_report.txt'\n",
        "\n",
        "write_txt_header(reward_txt_file,num_episodes,batch_size,initial_investment)\n",
        "\n",
        "for gamma,epsilon, mode in exploreHyper:  \n",
        "  colabEnv = mode\n",
        "  models_folder = f'/content/drive/My Drive/Colab Notebooks/rl/final/linear_rl_trader_random_models_{gamma}_{epsilon}'\n",
        "  rewards_folder = f'/content/drive/My Drive/Colab Notebooks/rl/final/linear_rl_trader_random_rewards_{gamma}_{epsilon}'\n",
        "\n",
        "  maybe_make_dir(models_folder)\n",
        "  maybe_make_dir(rewards_folder)\n",
        "  \n",
        "  # Read the CSV file and por to a numpy dataframe\n",
        "  data = get_data(file)\n",
        "\n",
        "  n_timesteps, n_stocks = data.shape\n",
        "\n",
        "  # use 50% of the data for train and test\n",
        "  n_train = n_timesteps // 2\n",
        "\n",
        "  train_data = data[:n_train]\n",
        "  test_data = data[n_train:]\n",
        "\n",
        "  env = MultiStockEnv(train_data, initial_investment)\n",
        "  state_size = env.state_dim\n",
        "  action_size = len(env.action_space)\n",
        "  agent = DQNAgent(state_size, action_size, gamma, epsilon)\n",
        "  scaler = get_scaler(env)\n",
        "\n",
        "  # store the final value of the portfolio (end of episode)\n",
        "  portfolio_value = []\n",
        "\n",
        "  if (colabEnv =='test'):\n",
        "    # then load the previous scaler\n",
        "    #with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
        "    #  scaler = pickle.load(f)\n",
        "\n",
        "    # remake the env with test data\n",
        "    env = MultiStockEnv(test_data, initial_investment)\n",
        "\n",
        "    # make sure epsilon is not 1!\n",
        "    # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
        "    #agent.epsilon = 0.01 \n",
        "\n",
        "    # load trained weights colab pd.read_csv\n",
        "    #agent.load(f'{models_folder}/linear.npz')\n",
        "\n",
        "  # play the game num_episodes times\n",
        "  for e in range(num_episodes):\n",
        "    t0 = datetime.now()\n",
        "    val = play_one_episode(agent, env, colabEnv)\n",
        "    dt = datetime.now() - t0\n",
        "    #print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\n",
        "    portfolio_value.append(val) # append episode end portfolio value\n",
        "\n",
        "  # save the weights when we are done\n",
        "  #if (colabEnv =='train'):\n",
        "    # save the DQN\n",
        "    #agent.save(f'{models_folder}/linear.npz')\n",
        "\n",
        "    # save the scaler\n",
        "    #with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
        "    #  pickle.dump(scaler, f)\n",
        "\n",
        "    # plot losses\n",
        "    #plt.plot(agent.model.losses)\n",
        "    #plt.savefig(f'{rewards_folder}/{mode}-losses.png')\n",
        "    #plt.show()\n",
        "    #plt.clf()\n",
        "\n",
        "\n",
        "  # save portfolio value for each episode\n",
        "  np.save(f'{rewards_folder}/{mode}.npy', portfolio_value)\n",
        "\n",
        "\n",
        "  # =============================================================\n",
        "  # Print reward charts\n",
        "  # =============================================================\n",
        "\n",
        "  url = f'{rewards_folder}/{mode}.npy'\n",
        "  a = np.load(url)\n",
        "\n",
        "  text = f\"{mode}-{gamma}-{epsilon} - average reward: {a.mean():.2f}, min: {a.min():.2f}, max: {a.max():.2f}\"\n",
        "  print(text)\n",
        "\n",
        "  f=open(reward_txt_file,\"a+\")\n",
        "  f.write(text)\n",
        "  f.write(\"\\r\\n\")\n",
        "  txt_now = f'{datetime.now()}'\n",
        "  f.write(txt_now)\n",
        "  print(txt_now)    \n",
        "  f.write(\"\\r\\n\")  \n",
        "  f.close()\n",
        "\n",
        "  plt.hist(a, bins=20)\n",
        "  plt.title(mode)\n",
        "  # Prepare to save the figure\n",
        "  fig1 = plt.gcf()\n",
        "  plt.show()\n",
        "\n",
        "  fig1.savefig(f'{rewards_folder}/{mode}-{a.mean()}.png')\n",
        "  plt.clf()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "train-0.95-0.95 - average reward: 29164.71, min: 12772.88, max: 59095.46\n",
            "2019-12-07 01:51:53.537447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test-0.95-0.95 - average reward: 25474.98, min: 12004.53, max: 49883.70\n",
            "2019-12-07 01:55:34.668414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHXKgJekThnwktSp/w+YUefNs4DxUwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fe2dada42a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolabEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m#print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-13dcabeb525e>\u001b[0m in \u001b[0;36mplay_one_episode\u001b[0;34m(agent, env, is_train)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#action = agent.act(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use random for comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-819c10392738>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# perform the trade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# get the new value after taking the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-819c10392738>\u001b[0m in \u001b[0;36m_trade\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuy_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcash_in_hand\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstock_price\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstock_owned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# buy one share\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcash_in_hand\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstock_price\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx7kQpXb0igY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}